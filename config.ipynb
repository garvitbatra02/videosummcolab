{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJsC26OIx1p+SlDyVg5Acw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyegupta12/myVASnet/blob/main/config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8_xJWLgBora",
        "outputId": "28de4412-b972-4dbe-f8b9-cdf6c1734161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] cuda_device: 0\n",
            "[1] datasets: ['datasets/eccv16_dataset_summe_google_pool5.h5', 'datasets/eccv16_dataset_tvsum_google_pool5.h5', 'datasets/eccv16_dataset_ovp_google_pool5.h5', 'datasets/eccv16_dataset_youtube_google_pool5.h5']\n",
            "[2] epochs_max: 300\n",
            "[3] l2_req: 1e-05\n",
            "[4] lr: [5e-05]\n",
            "[5] lr_epochs: [0]\n",
            "[6] max_summary_length: 0.15\n",
            "[7] output_dir: ex-10\n",
            "[8] root: \n",
            "[9] splits: ['splits/tvsum_splits.json', 'splits/summe_splits.json', 'splits/tvsum_aug_splits.json', 'splits/summe_aug_splits.json']\n",
            "[10] train_batch_size: 1\n",
            "[11] use_cuda: True\n",
            "[12] verbose: False\n",
            "\n",
            "[0] cuda_device: 0\n",
            "[1] datasets: ['set1,set2,set3']\n",
            "[2] epochs_max: 300\n",
            "[3] l2_req: 1e-05\n",
            "[4] lr: [5e-05]\n",
            "[5] lr_epochs: [0]\n",
            "[6] max_summary_length: 0.15\n",
            "[7] new_param_float: 1.23456\n",
            "[8] output_dir: ex-10\n",
            "[9] root: root_dir\n",
            "[10] splits: ['split1,', 'split2']\n",
            "[11] train_batch_size: 1\n",
            "[12] use_cuda: True\n",
            "[13] verbose: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class HParameters:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.verbose = False\n",
        "        self.use_cuda = True\n",
        "        self.cuda_device = 0\n",
        "        self.max_summary_length = 0.15\n",
        "\n",
        "        self.l2_req = 0.00001\n",
        "        self.lr_epochs = [0]\n",
        "        self.lr = [0.00005]\n",
        "\n",
        "        self.epochs_max = 300\n",
        "        self.train_batch_size = 1\n",
        "\n",
        "        self.output_dir = 'ex-10'\n",
        "\n",
        "        self.root = ''\n",
        "        self.datasets=['datasets/eccv16_dataset_summe_google_pool5.h5',\n",
        "                       'datasets/eccv16_dataset_tvsum_google_pool5.h5',\n",
        "                       'datasets/eccv16_dataset_ovp_google_pool5.h5',\n",
        "                       'datasets/eccv16_dataset_youtube_google_pool5.h5']\n",
        "\n",
        "        self.splits = ['splits/tvsum_splits.json',\n",
        "                        'splits/summe_splits.json']\n",
        "\n",
        "        self.splits += ['splits/tvsum_aug_splits.json',\n",
        "                        'splits/summe_aug_splits.json']\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def get_dataset_by_name(self, dataset_name):\n",
        "        for d in self.datasets:\n",
        "            if dataset_name in d:\n",
        "                return [d]\n",
        "        return None\n",
        "\n",
        "    def load_from_args(self, args):\n",
        "        for key in args:\n",
        "            val = args[key]\n",
        "            if val is not None:\n",
        "                if hasattr(self, key) and isinstance(getattr(self, key), list):\n",
        "                    val = val.split()\n",
        "\n",
        "                setattr(self, key, val)\n",
        "\n",
        "    def __str__(self):\n",
        "        vars = [attr for attr in dir(self) if not callable(getattr(self,attr)) and not (attr.startswith(\"__\") or attr.startswith(\"_\"))]\n",
        "\n",
        "        info_str = ''\n",
        "        for i, var in enumerate(vars):\n",
        "            val = getattr(self, var)\n",
        "            if isinstance(val, Variable):\n",
        "                val = val.data.cpu().numpy().tolist()[0]\n",
        "            info_str += '['+str(i)+'] '+var+': '+str(val)+'\\n'\n",
        "\n",
        "        return info_str\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Tests\n",
        "    hps = HParameters()\n",
        "    print(hps)\n",
        "\n",
        "    args = {'root': 'root_dir',\n",
        "            'datasets': 'set1,set2,set3',\n",
        "            'splits': 'split1, split2',\n",
        "            'new_param_float': 1.23456\n",
        "            }\n",
        "\n",
        "    hps.load_from_args(args)\n",
        "    print(hps)"
      ]
    }
  ]
}